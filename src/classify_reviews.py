# -*- coding: utf-8 -*-
"""Classify_reviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bf2fcqP0xE2h_tIVOPfmeRaQGqHlLENw
"""



"""# Classification of Genuine and Unnatural Reviews

About the data:

* Yelp hotel data collected by Dr. Bing Liu at University of Illinois at Chicago (UIC). It is stored in a SQLite database format.
* 688,095 reviews 

    - 5857 reviewers wrote their reviews on 72 hotels

    - Majority of these reviewers also wrote reviews on other eCommerce businesses at Yelp

* This is a labled dataset with genuine and filtered/fake reviews labeled as 'N' and 'Y' respectively. They are the target for this project.
"""

# Standard libraries and modules
import pickle
import pandas as pd
import numpy as np

# Libraries/tools for NLP tasks
import nltk
import re
import string
from textblob import TextBlob

# Modules for feature extraction
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# Preprocessing modules
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

# Machine learning and metrics
from sklearn import metrics
from sklearn.metrics import silhouette_score, confusion_matrix
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.linear_model import LogisticRegression

# Download file from colab
from google.colab import files

# Visualization
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud, ImageColorGenerator
from PIL import Image
# %matplotlib inline

#!pip install pandas==0.24.0

#!pip install numpy==1.15.4

#!pip install -U nltk
#import nltk
#nltk.download('all')

"""## Data Pre-processing"""

# The data are retrived from the database with SQLite. Relevant fields were 
#    extracted and stored in the pickle files. Loading the pickle file.

# Merge hotels' information and reviews, 
#    cleaned date format, drop unnecessary columns
with open('df_yh_review_NY.pickle', 'rb') as f:
  df_yh_review = pickle.load(f)
   
# Merge eCommerce businesses information and their reviews 
#    cleaned date format, drop unnecessary columns
#with open('df_yh_review_NRYR.pickle', 'rb') as f:
#    df_yh_review_NRYR = pickle.load(f)

# Check the hotel data (corpus). 
#    There are no null entries. 
#    There are 5857 documents and 11 columns.
df_yh_review.info()

# Look at the data.
df_yh_review.head(1)

# Remove empty cells that aren't detected by pandas 
df_yh_review['reviewContent'].replace('', np.nan, inplace=True)
df_yh_review.dropna(subset=['reviewContent'], inplace=True)

# Change the reviews' labels 'N' and 'Y' to 'G' and 'F' respectively. 
#    'N' - Not filtered, genuine reviews
#    'F' - Filtered, fake reviews
df_yh_review['flagged'] = df_yh_review['flagged'].str.replace('Y', 'F', regex=True)
df_yh_review['flagged'] = df_yh_review['flagged'].str.replace('N', 'G', regex=True)

# The total number of hotels
pd.unique(df_yh_review['name']).shape

# Sort the documents by date for hotel data
#    Date ranges from Oct 2004 to Sept 2012
df_yh_review['date'].sort_values()

# Check the data of other reviews. Are there any nulls?
df_yh_review_NRYR.info()

"""## Data Exploration"""

# 5857 documents (unique reviewers) and 72 hotels
# Check whether the fake reviewers abuse the rating button
# Looks like class imbalanced problem

sns.set()
sns.countplot(x='rating_x', hue='flagged', data=df_yh_review)
plt.xlabel('rating',fontsize=16)
plt.ylabel('count',fontsize=16)
plt.legend(fontsize='14')
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
#plt.savefig("reviewers_ratings.png", dpi=300)
#files.download( "reviewers_ratings.png" )

# The number of 
df_yh_review['flagged'].value_counts()

# The breakdown of reviews for each hotel
#   theWit hotel, Doubletree hotel and Old Chicago Inn have more fake reviews than genuine ones.
sns.set(rc={'figure.figsize':(16,14)})
df_yh_review.groupby(['name','flagged']).size().unstack(fill_value=0).plot(kind='bar', color=['seagreen','royalblue'])
plt.xlabel('hotel',fontsize=20)
plt.ylabel('count',fontsize=20)
plt.legend(fontsize='20')
plt.yticks(fontsize='18')
#plt.savefig("Hotel_numreviews.png", dpi=300)
#files.download( "Hotel_numreviews.png")

"""##Before the NLP processing steps
Gather features from the raw reviews (documents)
"""

# Extract feature: Count the punctuations
n_reviews = len(df_yh_review['reviewContent'])
punctuations = np.zeros(n_reviews)

for i in range(n_reviews):
    for c in df_yh_review['reviewContent'].iloc[i]:
        if c in string.punctuation:
            punctuations[i] += 1

punctuations

# Part of speech
#     Extract feature: count the classes (Noun, Verb, Adverb, Adjective, preposition, and Pronoun)
# References: 
#   http://rwet.decontextualize.com/book/textblob/
#   https://textblob.readthedocs.io/en/dev/quickstart.html

'''
NN: noun
NNS: noun, plural
NNP: noun, proper singular
VB: verb, base form	think 
VBZ: verb, 3rd person singular present (she thinks)
VBP: verb, non-3rd person singular present (I think) 
VBD: verb, past tense (they thought) 
VBN: verb, past participle (a sunken ship)
VBG: verb, gerund or present participle (thinking is fun)
RB: adverb (extremely, loudly, hard)
RBR: adverb, comparative (better) 
RBS: adverb, superlative (best) 
JJ: adjective
JJR: adjective, comparative
JJS: adjective, superlative
IN: preposition
PRP: pronoun, personal (me, you, it)
PRP: pronoun, possessive (my, your, our) 
'''

def posttag_textblob(sentences):
    '''
    '''
    blob = TextBlob(sentences)

    #[NN, VB, RB, JJ, IN, PR]
    postag = np.zeros(6)

    for word, pos in blob.tags:
        if 'NN' in pos:
            postag[0] += 1
        elif 'VB' in pos:
            postag[1] += 1            
        elif 'RB' in pos:
            postag[2] += 1            
        elif 'JJ' in pos:
            postag[3] += 1            
        elif 'IN' in pos:
            postag[4] += 1            
        elif 'PR' in pos:
            postag[5] += 1        
        #print(word, pos)
    return postag
  
post = []
for sentence in df_yh_review['reviewContent']:
    post.append(list(posttag_textblob(sentence)))

post[1:3]

"""##NLP processing steps 
Clean the data

Remove url in "reviewContent"
"""

# Remove urls
df_yh_review['reviewContent'] = df_yh_review['reviewContent'].str.replace(r'https?://\S+', '')
# Remove numbers 
df_yh_review['reviewContent'] = df_yh_review['reviewContent'].str.replace(r'\w*\d\w*', '')
# Tokenizatoin
newcolumn = 'tokenized_review'
df_yh_review[newcolumn] = df_yh_review['reviewContent'].apply(nltk.tokenize.word_tokenize)

# Check the 'tokenized_review' column
df_yh_review[newcolumn].head(10)

# Remove punctuations
df_yh_review[newcolumn] = df_yh_review[newcolumn].apply(lambda x: [s.translate(str.maketrans('','',string.punctuation)) for s in x])
# Remove isolated (1-2) character(s) 
df_yh_review[newcolumn] = df_yh_review[newcolumn].apply(lambda x: [re.sub(r'\b\w{,2}\b','', s) for s in x])
# Replace tabs, newlines, extra spacing with a space
df_yh_review[newcolumn] = df_yh_review[newcolumn].apply(lambda x: [s for s in x if s != ''])
df_yh_review[newcolumn] = df_yh_review[newcolumn].apply(lambda x: [s.replace(r'^\s+|\s+|\s+$', ' ') for s in x])
# Change texts to lower case
df_yh_review[newcolumn] = df_yh_review[newcolumn].apply(lambda x: [s.lower() for s in x])

# Check the 'tokenized_review' column
df_yh_review[newcolumn].head(10)

"""Apply Stemming (Note: Test lematization to see which one is better)

Reference: 

https://stackoverflow.com/questions/1787110/what-is-the-true-difference-between-lemmatization-vs-stemming

https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/

https://stackoverflow.com/questions/17317418/stemmers-vs-lemmatizers
"""

# Make wordcloud image for fake reviews
F_token = df_yh_review[df_yh_review['flagged']=='F']
slist =[]
for x in R_token:
    slist.extend(x)
text = ' '.join(slist)

wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
#plt.savefig("Filteredreviewers_wordcloud.png", dpi=300)
#files.download( "Filteredreviewers_wordcloud.png" )

# Make wordcloud image for genuine reviews
R_token = df_yh_review[df_yh_review['flagged']=='G']["tokenized_review"]
slist =[]
for x in R_token:
    slist.extend(x)
text = ' '.join(slist)  

wordcloud = WordCloud(background_color='white').generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.margins(x=0, y=0)
#plt.savefig("reviewers_wordcloud.png", dpi=300)
#files.download( "reviewers_wordcloud.png" )

# Apply NLTK's stemmer 
#   It trims the words, so that they are converted back to their base form but 
#    other words that shouldn't be trimed aren't being trimed as well.

#stemmer = nltk.stem.SnowballStemmer('english')
##df_yh_review['reviewContent'] = df_yh_review['reviewContent'].apply(lambda x: stemmer.stem(x))
#df_yh_review[newcolumn] = df_yh_review[newcolumn].apply(lambda x: [stemmer.stem(s) for s in x])

# Apply lemmantization 
#    It doesn't apply for this project since I wanted the base form of the verbs

#lemmatizer = nltk.stem.WordNetLemmatizer()
##df_yh_review['reviewContent'] = df_yh_review['reviewContent'].apply(lambda x: lemmatizer.lemmatize(x))

# Spelling correction using TextBlob doesn't work because it breaks the word into characters.
#    Tried on AWS and Colab

#df_yh_review[newcolumn] = df_yh_review[newcolumn].apply(lambda x: [TextBlob(s).correct() for s in x])

# Remove stop words
stop_words = set(nltk.corpus.stopwords.words('english')) 
df_yh_review['tokenized_stop'] = df_yh_review[newcolumn].apply(lambda x: [s for s in x if s not in stop_words])

# Check to make sure stop words are removed
test = df_yh_review['tokenized_stop'].head(1)
with pd.option_context('display.max_colwidth', 5000):
    print (test)

"""## *Unigrams*: Frequency and Filtering"""

# Collect the unique unigrams
unique_words = list(set([s for row in df_yh_review['tokenized_stop'] for s in row]))
word_count = dict.fromkeys(unique_words, 0)
for word_list in df_yh_review['tokenized_stop']:
    for w in word_list:
      word_count[w] += 1

# Total number of unique unigrams
len(word_count)

# Look at the unigrams and their counts
df_1gram_freq = pd.DataFrame(word_count.items(), columns=['word', 'count'])
# Begining of the list
df_1gram_freq.sort_values('word').head()

# From the bottem of the sorted list
df_1gram_freq.sort_values('count').tail()

# See the frequency of words that contain 'zzz'
df_1gram_freq[df_1gram_freq['word'].str.contains('zzz')]

'''
 Look at the freqiency of the unique unigrams
 Determine filtering criteria 
'''

values = np.array([value for key, value in word_count.items()])
keys = [key for key, value in word_count.items()]

# Find out the number of words if it is < 3 and > 500
hifrqw = np.where(values > 500)
lofrqw = np.where(values < 3)
# 12877 words that have < 3 counts. These are considered rare words
print(len(lofrqw[0]))


delwords = []
for i in lofrqw[0]:
  delwords.append(keys[i])
for i in hifrqw[0]:
  delwords.append(keys[i])
print(delwords)

# Total number of words is 20911
print(len(values))
# 134 words that have > 500 counts
print(len(np.where(values > 500)[0]))
# Remaining number of unigrams
print(20911-12877-134)

# Plot the word freqency (normalized) of the words
# Nornalize the word frequency
v_nor = np.array(values)/max(values)
plt.plot(range(len(v_nor)),v_nor)
plt.xlabel('unique words index',fontsize=16)
plt.ylabel('frequency',fontsize=16)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
#plt.savefig("unigram_wordfreq.png", dpi=300)
#files.download( "unigram_wordfreq.png" )

# 13011 number of words to be deleted
len(delwords)

'''
Create a function to delete words in the documents from the 'delwords' list
'''

def delete_words(X, delwords):
  for w in X:
    if w in delwords:
      X.remove(w)
  return X

# Apply the  delete_words function to delete rare and high frequency words
df_yh_review['tokenized_stop'] = df_yh_review['tokenized_stop'].apply(lambda x: delete_words(x,delwords))

"""## *Bigrams*: Frequency and Filtering"""

'''
Apply NLP processing step excluding stemmer 

'''

newcolumn2 = '2_grams'
# Tokenization
df_yh_review[newcolumn2] = df_yh_review['reviewContent'].apply(nltk.tokenize.word_tokenize)
# Remove punctuation
df_yh_review[newcolumn2] = df_yh_review[newcolumn2].apply(lambda x: [s.translate(str.maketrans('','',string.punctuation)) for s in x])
# Remove isolated words with 1-2 characters
df_yh_review[newcolumn2] = df_yh_review[newcolumn2].apply(lambda x: [re.sub(r'\b\w{,2}\b','', s) for s in x])
# Remove any white space
df_yh_review[newcolumn2] = df_yh_review[newcolumn2].apply(lambda x: [s for s in x if s != ''])
df_yh_review[newcolumn2] = df_yh_review[newcolumn2].apply(lambda x: [s.replace(r'^\s+|\s+|\s+$', ' ') for s in x])
# Convert to low case                                                 
df_yh_review[newcolumn2] = df_yh_review[newcolumn2].apply(lambda x: [s.lower() for s in x])
# Generate bigrams
df_yh_review[newcolumn2] = df_yh_review[newcolumn2].apply(lambda x: list((nltk.ngrams(x,2))))

# Remove bigram that contain any stop word
stop_words = set(nltk.corpus.stopwords.words('english')) 
df_yh_review[newcolumn2] = df_yh_review[newcolumn2].apply(lambda x: [(i,j) for i,j in x if (i not in stop_words) and (j not in stop_words)])
df_yh_review[newcolumn2].head()

# Add '_' to join the words in each tuple
df_yh_review[newcolumn2] = df_yh_review[newcolumn2].apply(lambda x: ['_'.join([i,j]) for i,j in x])
df_yh_review[newcolumn2].head()

# Find unique bigrams and their counts 
test2 = df_yh_review[newcolumn2].apply(lambda x: " ".join(x))
word_vectorizer = CountVectorizer(analyzer='word')
sparse_matrix = word_vectorizer.fit_transform(test2)
frequencies = sum(sparse_matrix).toarray()[0]
df_test2 = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])
df_test2.index.name = 'bigrams'

# Total number of unique bigrams
len(df_test2['frequency'].index.values)

199130-196507

# Filter bigrams that have < 10 counts
del_2grams = list(df_test2[df_test2['frequency']<10].index.values)
del_2grams[4]

# Total number of bigrams to be deleted 
len(del_2grams)

# Apply the delete_words function to delete the low frequency bigrams 
df_yh_review[newcolumn2] = df_yh_review[newcolumn2].apply(lambda x: delete_words(x,del_2grams))
df_yh_review[newcolumn2].head()

# Look at the current df_yh_review dataframe
df_yh_review.head(1)

# Bag of words: Now combine the unigrams and bigrams
df_yh_review['combine'] =  df_yh_review['tokenized_stop'] + df_yh_review['2_grams']
df_yh_review.head()

"""##Applying Non-negative matrix factorization (NMF) to extract the topics. The weights become the features for classification."""

# Modified the display_topics function that is taken from the lecture notes
def display_topics(model, feature_names, no_top_words, topic_names=None):
    
    for ix, topic in enumerate(model.components_):
        print("\nTopic ", ix)
        print(", ".join([feature_names[i]
                    for i in topic.argsort()[:-no_top_words - 1:-1]]))

"""NMF - Split reviewers"""

# Generate topics from the filtered (fake) reviews

df_F = df_yh_review[df_yh_review['flagged']=='F']
df_test5 = df_F['combine'].apply(lambda x: " ".join(x))

# Generate the term-document matrix with TFIDF 
Tvectorizer = TfidfVectorizer(max_features=10000)
Tvectorizer.fit(df_test5)
term_doc_mtx = Tvectorizer.transform(df_test5)
labels = [e[:30]+"..." for e in df_test5]
df_tdm = pd.DataFrame(term_doc_mtx.toarray(), index=labels, columns=Tvectorizer.get_feature_names())

# Apply NMF
n_components = 10
nmf_model = NMF(n_components, init='random', random_state=0)
W = nmf_model.fit_transform(df_tdm)
# Display the topics (fake reviews)
display_topics(nmf_model, Tvectorizer.get_feature_names(), n_components)

# Generate topics from the genuine reviews
df_R = df_yh_review[df_yh_review['flagged']=='G']
df_test5 = df_R['combine'].apply(lambda x: " ".join(x))

# Generate the term-document matrix with TFIDF 
Tvectorizer = TfidfVectorizer(max_features=10000)
Tvectorizer.fit(df_test5)
term_doc_mtx = Tvectorizer.transform(df_test5)
labels = [e[:30]+"..." for e in df_test5]
df_tdm = pd.DataFrame(term_doc_mtx.toarray(), index=labels, columns=Tvectorizer.get_feature_names())

# Apply NMF
n_components = 10
nmf_model = NMF(n_components, init='random', random_state=0)
W = nmf_model.fit_transform(df_tdm)
# Display the topics (genuine reviews)
display_topics(nmf_model, Tvectorizer.get_feature_names(), n_components)

"""Now, find the topics and theoir weights for all the reviews"""

# All the reviews
# Generate the term-document matrix with TFIDF 
Tvectorizer = TfidfVectorizer(max_features=10000)
df_test5 = df_yh_review['combine'].apply(lambda x: " ".join(x))
Tvectorizer.fit(df_test5)
term_doc_mtx = Tvectorizer.transform(df_test5)
labels = [e[:30]+"..." for e in df_test5]
df_tdm = pd.DataFrame(term_doc_mtx.toarray(), index=labels, columns=Tvectorizer.get_feature_names())

df_tdm.head(3)

# All the reviews
# Apply NMF
n_components = 10
nmf_model = NMF(n_components, init='random', random_state=0)
W = nmf_model.fit_transform(df_tdm)

# All the reviews
# Look at the H matrix
H = nmf_model.components_
topic_word = pd.DataFrame(H.round(3),
             index = ['component_'+str(i+1) for i in range(n_components)],
             columns = Tvectorizer.get_feature_names())
topic_word

# Display the topics
display_topics(nmf_model, Tvectorizer.get_feature_names(), n_components)

# All the reviews
# Convert the Weight matrix to a dataframe. These 10 components will be the 
# part of the features for classifier
W_M = pd.DataFrame(W.round(5),
             index = labels,
             columns = ['component_'+str(i+1) for i in range(n_components)])
W_M

# Check the weight's matrix dimention
W_M.shape

# Merge part of speech's classes and puntuation with weight matrix to generate
#   the feature (X) for classifier
df_fea = pd.DataFrame(data=np.array(post), index = labels,
             columns = ['noun', 'verb', 'adverb', 'adjective', 'preposition', 'pronoun'])
df_fea['punctuation'] = punctuations
df_fea = pd.concat([W_M, df_fea], axis=1)
df_fea.head()

# The matrix size of the feature matrix
df_fea.shape

# See the statistic of the other features (POS and punctuation) 
#    Add the other features to the existing dataframa
A = df_yh_review.copy()
B = pd.DataFrame(data=np.array(post),
             columns = ['noun', 'verb', 'adverb', 'adjective', 'preposition', 'pronoun'])
B['punctuation'] = punctuations
A = pd.concat([A, B], axis=1)

# Show the means of the features for 'F' and 'G'
Xcat = ['noun', 'verb', 'adverb', 'adjective', 'preposition', 'pronoun', 'punctuation']
sel_df = A[['flagged','noun', 'verb', 'adverb', 'adjective', 'preposition', 'pronoun', 'punctuation']]
sel_df.groupby(["flagged"]).mean()

"""## Unsupervised clustering"""

# Get the targets
y = df_yh_review['flagged']
# Convert to categorical encoding
y = pd.DataFrame(y.astype('category').cat.codes, columns=['flagged'])
y.reset_index(drop=True, inplace=True)

nrows,ncol = df_fea.shape
# 1757 documents are set as test set
ntest = round(0.3*nrows)
# The rest are for training the model
ntrain = nrows - ntest

df_fea_y = df_fea.reset_index(drop=True)
df_fea_y = pd.concat([df_fea_y, y], axis=1)
df_fea_shuffle = shuffle(df_fea_y, random_state=42)
X_train = df_fea_shuffle.loc[0:ntrain, df_fea_y.columns != 'flagged']
X_test = df_fea_shuffle.loc[ntrain:, df_fea_y.columns != 'flagged']
y_train = df_fea_shuffle.loc[0:ntrain, 'flagged']
y_test = df_fea_shuffle.loc[ntrain:, 'flagged']


std = StandardScaler()
std.fit(X_train.values)
X_tr = std.transform(X_train.values)
X_te = std.transform(X_test.values)


# Normalized the X 
#X_train = X_train.div(X_train.sum(axis=1), axis=0)

# Kmean clustering
k = 2
km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=100, random_state = 0) #, verbose=1)
km.fit(X_tr)


sscore = silhouette_score(X_tr, y_train, metric='euclidean', sample_size=None, random_state=0) 
print('Kmean - Inertia = ', km.inertia_)
print('Kmean - silhouette = ', sscore)

ypred_train = km.predict(X_tr)
print('Train: correct = ', 1-(sum(abs(ypred_train-y_train))/len(y_train)))
ypred_test = km.predict(X_te)
print('Test: correct = ', 1-(sum(abs(ypred_test-y_test))/len(y_test)))

# GaussianMizture (Imbalance class) - anamoly detection
# NOTE: Try Hierarchical Clustering - agglomerative clustering
# https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html
gmm = GaussianMixture(n_components=2)
gmm.fit(X_tr)

ypred_train = gmm.predict(X_tr)
print('Train: correct = ', 1-(sum(abs(ypred_train-y_train))/len(y_train)))
ypred_test = gmm.predict(X_te)
print('Test: correct = ', 1-(sum(abs(ypred_test-y_test))/len(y_test)))

#print(gmm.means_)
#print(gmm.covariances_)

# PCA to make artificial axis
# plot the Kmean and GMM results

"""## Classification"""

# Get the targets
y = df_yh_review['flagged']
# Convert to categorical encoding
y = y.astype('category').cat.codes

# Prepare training, validation, and test datasets for Logistic Regression
X_train_val, X_test, y_train_val, y_test1 = \
        train_test_split(df_fea, y, test_size=0.3, stratify=y, random_state=42)
        
X_train, X_val, y_train, y_val = \
        train_test_split(X_train_val, y_train_val, test_size=0.3, \
                         stratify=y_train_val, random_state=43)
        
# Apply Standard Scaler to training and validation sets
std = StandardScaler()
std.fit(X_train.values)    
X_train_scaled = std.transform(X_train.values)
X_val_scaled = std.transform(X_val.values)
X_test_scaled = std.transform(X_test.values) 

# # Imbalanced class: Create an oversampling instance
sm = SMOTE(random_state=42)

# Logistic Regression
lr = LogisticRegression()             
X_r, y_r = sm.fit_sample(X_train_scaled,y_train)                   
lr.fit(X_r,y_r)
ypred = lr.predict(X_val_scaled)
                
# Confusion matrix
cm = metrics.confusion_matrix(y_val,ypred)         
print(classification_report(y_val, ypred))                    

# Make a confusion matrix heatmap
df_cm = pd.DataFrame(cm, index=['Filter','Real'], columns=['Filter','Real'])
sns.set()
sns.heatmap(df_cm, annot=True, fmt="d", annot_kws={"size": 18})
plt.ylabel('Actual', fontsize='18')
plt.xlabel('Predict', fontsize='18')
plt.xticks(fontsize='16')
plt.yticks(fontsize='16')
#plt.savefig("CM_logisticR.png", dpi=300)
#files.download( "CM_logisticR.png" )